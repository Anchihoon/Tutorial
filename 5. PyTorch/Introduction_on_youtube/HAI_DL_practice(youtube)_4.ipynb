{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMG2nQ8iYH4l"
      },
      "source": [
        "# **4. Building Models with PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameter를 제외한 모든 클래스는 torch.nn.Module의 하위 클래스임 이 클래스는 파이토치 모델과 그 구성 요소에 특화도니 동작을 캡슐화 하도록 설계된 파이토치 기본 클래스임\n",
        "\n",
        "torch.nn.Module의 중요한 동작 중 하나는 파라미터 등록임\n",
        "\n",
        "만약 어떤 모듈 클래스가 학습 가능한 가중치를 가진다면 이 가중치는 torch.nn.Parameter 인스턴스로 표현됨"
      ],
      "metadata": {
        "id": "HX68oHIiZ-Gr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWiafHi5YCWI",
        "outputId": "6e422de4-4e29-4cc9-a6d9-f89e79dfe4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            " Just One layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            " Model Params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0253,  0.0231,  0.0601,  ..., -0.0821, -0.0310,  0.0845],\n",
            "        [ 0.0021,  0.0274, -0.0558,  ...,  0.0193,  0.0664,  0.0204],\n",
            "        [ 0.0284, -0.0784, -0.0757,  ...,  0.0590,  0.0711, -0.0973],\n",
            "        ...,\n",
            "        [-0.0264, -0.0486, -0.0788,  ..., -0.0181, -0.0014,  0.0489],\n",
            "        [ 0.0738,  0.0836,  0.0392,  ..., -0.0027, -0.0815,  0.0357],\n",
            "        [ 0.0312,  0.0549, -0.0012,  ...,  0.0999, -0.0534, -0.0494]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0739, -0.0076, -0.0209,  0.0937, -0.0022,  0.0037, -0.0568,  0.0324,\n",
            "        -0.0621, -0.0646,  0.0314,  0.0666, -0.0882, -0.0183, -0.0636, -0.0690,\n",
            "         0.0441,  0.0002, -0.0788,  0.0364, -0.0473, -0.0353, -0.0062,  0.0460,\n",
            "         0.0722,  0.0538,  0.0304, -0.0541, -0.0638, -0.0203,  0.0532, -0.0138,\n",
            "         0.0773,  0.0071,  0.0381,  0.0700,  0.0495,  0.0760,  0.0405,  0.0100,\n",
            "         0.0109,  0.0445, -0.0481, -0.0332, -0.0555, -0.0627, -0.0983,  0.0491,\n",
            "        -0.0436, -0.0379, -0.0224, -0.0735,  0.0172, -0.0344, -0.0972,  0.0902,\n",
            "         0.0310, -0.0139,  0.0922, -0.0394,  0.0385, -0.0349, -0.0969,  0.0921,\n",
            "        -0.0340,  0.0973, -0.0148, -0.0748, -0.0619, -0.0766,  0.0849,  0.0120,\n",
            "        -0.0873, -0.0636, -0.0373,  0.0606,  0.0850, -0.0235, -0.0609, -0.0096,\n",
            "         0.0199, -0.0113, -0.0091,  0.0087,  0.0608,  0.0451,  0.0127,  0.0456,\n",
            "         0.0261,  0.0857, -0.0525,  0.0797,  0.0400, -0.0289, -0.0738,  0.0532,\n",
            "        -0.0506, -0.0646, -0.0088, -0.0547,  0.0992,  0.0229,  0.0006,  0.0027,\n",
            "         0.0692, -0.0429, -0.0691,  0.0376,  0.0457, -0.0181, -0.0619,  0.0973,\n",
            "        -0.0722, -0.0750, -0.0483,  0.0683, -0.0312,  0.0557,  0.0103,  0.0942,\n",
            "         0.0794,  0.0066, -0.0709, -0.0302, -0.0232, -0.0148, -0.0127,  0.0300,\n",
            "         0.0522, -0.0188,  0.0906,  0.0184,  0.0062,  0.0601,  0.0172,  0.0278,\n",
            "        -0.0228,  0.0850,  0.0250,  0.0625,  0.0117,  0.0047, -0.0057,  0.0262,\n",
            "        -0.0350,  0.0808,  0.0274, -0.0720, -0.0441, -0.0052,  0.0016,  0.0458,\n",
            "         0.0980,  0.0802, -0.0395,  0.0717,  0.0554,  0.0263,  0.0481, -0.0543,\n",
            "         0.0770, -0.0034,  0.0177,  0.0187, -0.0934, -0.0688,  0.0348, -0.0461,\n",
            "        -0.0416,  0.0957, -0.0323,  0.0412,  0.0944, -0.0014, -0.0351, -0.0823,\n",
            "        -0.0044, -0.0859, -0.0099, -0.0933, -0.0178, -0.0113, -0.0811,  0.0068,\n",
            "         0.0657, -0.0393,  0.0037, -0.0995, -0.0517, -0.0718, -0.0813, -0.0896,\n",
            "         0.0273, -0.0536, -0.0478,  0.0214,  0.0590, -0.0325,  0.0157,  0.0125],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0600, -0.0676, -0.0056,  ...,  0.0453, -0.0600, -0.0225],\n",
            "        [-0.0351, -0.0411,  0.0289,  ..., -0.0104, -0.0042, -0.0496],\n",
            "        [ 0.0478,  0.0182,  0.0112,  ...,  0.0300,  0.0465, -0.0313],\n",
            "        ...,\n",
            "        [-0.0035, -0.0052,  0.0630,  ..., -0.0613,  0.0367, -0.0682],\n",
            "        [-0.0426,  0.0408,  0.0253,  ..., -0.0100, -0.0595, -0.0327],\n",
            "        [ 0.0039,  0.0492, -0.0579,  ..., -0.0100, -0.0410, -0.0307]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0054,  0.0144,  0.0148, -0.0470,  0.0257,  0.0689, -0.0201, -0.0648,\n",
            "        -0.0493,  0.0148], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0600, -0.0676, -0.0056,  ...,  0.0453, -0.0600, -0.0225],\n",
            "        [-0.0351, -0.0411,  0.0289,  ..., -0.0104, -0.0042, -0.0496],\n",
            "        [ 0.0478,  0.0182,  0.0112,  ...,  0.0300,  0.0465, -0.0313],\n",
            "        ...,\n",
            "        [-0.0035, -0.0052,  0.0630,  ..., -0.0613,  0.0367, -0.0682],\n",
            "        [-0.0426,  0.0408,  0.0253,  ..., -0.0100, -0.0595, -0.0327],\n",
            "        [ 0.0039,  0.0492, -0.0579,  ..., -0.0100, -0.0410, -0.0307]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0054,  0.0144,  0.0148, -0.0470,  0.0257,  0.0689, -0.0201, -0.0648,\n",
            "        -0.0493,  0.0148], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TinyModel, self).__init__()\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(100, 200)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(200, 10)\n",
        "    self.softmax = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('Model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\n Just One layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\n Model Params:')\n",
        "for param in tinymodel.parameters():\n",
        "  print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "  print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Layer Types"
      ],
      "metadata": {
        "id": "u5ePND3ralTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "신경망의 가장 기본적인 유형의 계층은 선형 계층 (Linear layer) 또는 Fully connected layer 임\n",
        "\n",
        "이 계층은 모든 입력이 출력의 모든 요소에 영향을 미치며 가중치로 그 영향이 결정될거임\n",
        "\n"
      ],
      "metadata": {
        "id": "fzMqnoBjsR21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ZqbJsXZnL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a16680-a2ff-4781-9c8c-6f7f05fdd166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[0.0604, 0.3293, 0.3864]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 0.5087,  0.2750, -0.2794],\n",
            "        [-0.5630,  0.5327, -0.0600]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.4664, -0.1808], requires_grad=True)\n",
            "\n",
            "\n",
            "Output:\n",
            "tensor([[ 0.4797, -0.0625]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 선형 계층, 입력차원 3 / 출력차원 2\n",
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3) # 입력 텐서\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters(): # 선형 계층의 파라미터 출력\n",
        "  print(param)\n",
        "\n",
        "# 선형 계층 통과\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y) # 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution Layers"
      ],
      "metadata": {
        "id": "YdviYxuuvuXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "합성곱 계층은 이미지의 특징을 인식하기 위해 자주 사용"
      ],
      "metadata": {
        "id": "kX_At4Qsvyqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = torch.nn.Conv2d(1,6,5) # 입력 채널 1, 출력 채널 6, 커널 크기 5 x\n",
        "    self.conv2 = torch.nn.Conv2d(6, 16, 3) # 입력 채널 6, 출력 채널 16, 커널 크기 3 x 3\n",
        "    self.fc1 = torch.nn.Linear(16 * 6 * 6, 120) # 입력 576, 출력 120\n",
        "    self.fc2 = torch.nn.Linear(120, 84)\n",
        "    self.fc3 = torch.nn.Linear(84, 10) # 10개 클래스\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  # Flatten 시에 필요한 feature 수 계산\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:] # 배치 차원 제외\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features"
      ],
      "metadata": {
        "id": "Ea91PgGma2yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrnet Layers"
      ],
      "metadata": {
        "id": "dLTd2npYw-y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN은 시퀀스 데이터에 적합\n",
        "\n",
        "RNN은 시퀀스를 따라 기억을 유지하면서 다음 단계로 전달함\n"
      ],
      "metadata": {
        "id": "Ld56Z4iBxFAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # 단어 임베딩 정의 (어휘 수, 임베딩 차원)\n",
        "    self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # LSTM 계층 : 임베딩을 입력으로 받아 hidden_dim 크기의 상태 출력\n",
        "    self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "    # 출력층 : hidden state -> tag 분류\n",
        "    self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    # 임베딩 처리 : 각 단어를 임베딩 벡터로 매핑\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "\n",
        "    # LSTM에 입력 -> (시퀀스 길이, 배치, 임베딩 차원)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "\n",
        "    # hidden2tag : (시퀀스 길이, 태그 수)\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "\n",
        "    # Log_softmax로 확률 출\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "metadata": {
        "id": "NGzdW4dpumE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation Layers"
      ],
      "metadata": {
        "id": "1Rz0VvoMGECU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 모델에는 학습 가능 파라미터는 없지만 중요한 역할을 수행하는 계층들이 있음\n",
        "\n",
        "pooling layer에서는 텐서의 여러 셀을 하나로 합쳐 축소시키는 역할을 함\n",
        "\n"
      ],
      "metadata": {
        "id": "l48NsPcBw6kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 6 ,6) # 무작위 텐서 생성\n",
        "print(my_tensor)\n",
        "\n",
        "maxpool_layer = torch.nn.MaxPool2d(3) # 3 x 3 필터로 maxpooling 수행\n",
        "print(maxpool_layer(my_tensor)) # 2 x 2로 축소된 것 확인가능"
      ],
      "metadata": {
        "id": "f9FKWE9GvkH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f7cfb2-73fb-4e30-ca97-e4c954954fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0648, 0.6371, 0.9348, 0.8121, 0.1974, 0.6994],\n",
            "         [0.9895, 0.4340, 0.6323, 0.4568, 0.3168, 0.5022],\n",
            "         [0.3403, 0.6135, 0.0097, 0.0473, 0.1302, 0.0443],\n",
            "         [0.6599, 0.7203, 0.6938, 0.7566, 0.8212, 0.1226],\n",
            "         [0.8400, 0.3933, 0.3167, 0.6692, 0.4682, 0.9360],\n",
            "         [0.3327, 0.9790, 0.0202, 0.8790, 0.8615, 0.4834]]])\n",
            "tensor([[[0.9895, 0.8121],\n",
            "         [0.9790, 0.9360]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규화는 다음 계층에 전달하기 전에 출력을 정규화해서 학습을 안정화 시킴\n",
        "\n",
        "활성화 함수의 기울기가 0 근처에서 가장 크므로, 입력 분포를 0 주변으로 모으면 학습 속도와 안정성이 향상된다\n"
      ],
      "metadata": {
        "id": "a4_BF2tyydYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 평균 5, 표준편차 20 정도로 스케일된 텐서\n",
        "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
        "print(my_tensor)\n",
        "print(my_tensor.mean()) # 평균이 13임\n",
        "\n",
        "norm_layer = torch.nn.BatchNorm1d(4) # 채널 개수 4개로 배치 정규화\n",
        "normed_tensor = norm_layer(my_tensor)\n",
        "print(normed_tensor)\n",
        "print(normed_tensor.mean()) # 평균이 0에 가까워짐"
      ],
      "metadata": {
        "id": "lE0x1HXTFxYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bce40e-7017-4f37-bb3b-0ae67286102c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 9.7749, 12.3121,  7.5704,  5.7735],\n",
            "         [ 9.7683, 17.8252, 14.6655,  7.8687],\n",
            "         [20.3475, 11.1342, 22.9355,  9.1526],\n",
            "         [22.3060, 14.5677, 14.7394, 21.0780]]])\n",
            "tensor(13.8637)\n",
            "tensor([[[ 0.3749,  1.4119, -0.5262, -1.2606],\n",
            "         [-0.7022,  1.3450,  0.5421, -1.1849],\n",
            "         [ 0.7598, -0.8115,  1.2012, -1.1495],\n",
            "         [ 1.1655, -1.0165, -0.9681,  0.8192]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "tensor(4.8429e-08, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "드랍아웃은 학습 시에 입력 텐서의 일부 요소를 확률적으로 0으로 만들어 과적합을 줄이고 희소 표현을 유도. 인퍼런스 시에는 자동으로 비활성화\n"
      ],
      "metadata": {
        "id": "4A5l2Pi4zG58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor = torch.rand(1, 4, 4)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.4) # 40% 확률로 드랍아웃\n",
        "print(dropout(my_tensor)) # 일부 값들이 0으로 마스킹 된것을 확인가능\n",
        "print(dropout(my_tensor)) # 호출할 때마다 마스크가 바뀜"
      ],
      "metadata": {
        "id": "CLTiKkVhGAs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada609cd-498b-49f7-cf26-b26d38774440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.5612, 0.3873, 1.3936, 0.4430],\n",
            "         [0.0000, 0.7739, 1.0800, 0.0000],\n",
            "         [0.8307, 0.7257, 0.0000, 1.1206],\n",
            "         [0.0000, 0.3563, 0.0000, 0.5511]]])\n",
            "tensor([[[1.5612, 0.0000, 1.3936, 0.4430],\n",
            "         [1.1498, 0.7739, 1.0800, 0.6723],\n",
            "         [0.0000, 0.7257, 0.9893, 0.0000],\n",
            "         [0.0000, 0.0000, 0.8240, 0.5511]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "activation function, 즉 활성화 함수는 신경망에 선형 연산만 반복하게 되면 사실 네트워크 전체가 하나의 큰 행렬 곱으로 축약되어 버림\n",
        "\n",
        "비선형 활성화 함수를 층 사이에 삽입해야 복잡한 비선형 함수를 근사할 수 있음\n"
      ],
      "metadata": {
        "id": "b9oHdLxRzNSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "손실 함수는 모델 예측과 정답 사이의 오차를 수치화해 학습 방향을 제시함"
      ],
      "metadata": {
        "id": "I3HIjPaozaJh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW6-FDIyx9zJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}